#!/bin/bash
#SBATCH --job-name=can3tok_semantic
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --time=23:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --mem=64G
#SBATCH --output=logs/train_cannorm_500e_3.8k_11_nsi.out
#SBATCH --error=logs/can3tok_%j.err

# ============================================================================
# CAN3TOK TRAINING - FULL SEMANTIC CONTROL + COLOR WEIGHTING + SCALE MODE
# ============================================================================
#
# HOW TO CONTROL CHECKPOINT RESUMING:
# ─────────────────────────────────────────────────────────────────────────────
#
# Case 1 — Fresh training (default):
#   Set RESUME_CHECKPOINT="" (empty string)
#   → Starts from epoch 0 with random weights
#
# Case 2 — Resume from a specific checkpoint (most common):
#   Set RESUME_CHECKPOINT="/path/to/epoch_950.pth"
#   → Loads weights + optimizer state
#   → Automatically resumes from epoch 951
#   → best_val_loss is restored so good checkpoints are not overwritten
#   → New checkpoints saved to a NEW folder with current SLURM job ID
#
# Case 3 — Fine-tune / reset epoch counter:
#   Set RESUME_CHECKPOINT="/path/to/best_model.pth"
#   Set RESUME_EPOCH=0
#   → Loads weights + optimizer state
#   → Epoch counter starts at 0 (treats it like a fresh run from pretrained weights)
#   → Useful for ablation experiments starting from the same pretrained base
#
# Available checkpoint files in each run folder:
#   best_model.pth  → best validation L2 recorded during that run
#   epoch_N.pth     → saved every 50 epochs starting from epoch 10
#   final.pth       → saved at end of training
#
# ─────────────────────────────────────────────────────────────────────────────

echo "================================================================================"
echo "               CAN3TOK TRAINING - SEMANTIC LEARNING CONTROL"
echo "================================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Start time: $(date)"
echo ""

# ============================================================================
# CHECKPOINT RESUMING CONFIGURATION   ← CONTROL THIS
# ============================================================================

# Set to empty string "" for fresh training.
# Set to a full path to resume from a saved checkpoint.
RESUME_CHECKPOINT="/home/yli11/scratch/Hafeez_thesis/Can3Tok/checkpoints/RGB_job_20043776_none/final.pth"

# Optional: override epoch counter. Leave empty to auto-detect from checkpoint.
# Set to a number (e.g. "0") to reset the epoch counter (for fine-tuning).
RESUME_EPOCH="1000"

# ============================================================================
# SEMANTIC LEARNING CONFIGURATION
# ============================================================================

SEMANTIC_MODE="geometric"
SEGMENT_WEIGHT=0.0
INSTANCE_WEIGHT=0.0

# Other options (uncomment to use):
# SEMANTIC_MODE="geometric"
# SEGMENT_WEIGHT=0.3
# INSTANCE_WEIGHT=0.0

# SEMANTIC_MODE="hidden"
# SEGMENT_WEIGHT=0.3
# INSTANCE_WEIGHT=0.0

# SEMANTIC_MODE="attention"
# SEGMENT_WEIGHT=0.3
# INSTANCE_WEIGHT=0.0

SEMANTIC_TEMP=0.1
SEMANTIC_SUBSAMPLE=10000
SAMPLING_STRATEGY="balanced"

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================

BATCH_SIZE=64
NUM_EPOCHS=1200
LEARNING_RATE=1e-5
KL_WEIGHT=1e-6
EVAL_EVERY=20
FAILURE_THRESHOLD=100

# ============================================================================
# COLOR LOSS WEIGHTING
# ============================================================================
COLOR_LOSS_WEIGHT=1.0

# ============================================================================
# SCALE NORMALIZATION MODE
# ============================================================================
SCALE_NORM_MODE="linear"   # "log" | "linear"

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

TRAIN_SCENES=300
VAL_SCENES=30
SAMPLING_METHOD="opacity"

# ============================================================================
# PCA VISUALIZATION CONFIGURATION
# ============================================================================

PCA_VIS_FREQ=10
PCA_NUM_SCENES=3
PCA_BRIGHTNESS=1.25

# ============================================================================
# 3DGS RECONSTRUCTION CONFIGURATION
# ============================================================================

RECON_PLY_FREQ=10
RECON_PLY_NUM_SCENES=3
RECON_PLY_MAX_SH=3

# ============================================================================
# NORMALIZATION CONTROL
# ============================================================================

USE_CANONICAL_NORM=True

# ============================================================================
# WEIGHTS & BIASES
# ============================================================================

export WANDB_API_KEY="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"
USE_WANDB=False

# ============================================================================
# PRINT CONFIGURATION SUMMARY
# ============================================================================

echo "========================================================================"
echo "                     CONFIGURATION SUMMARY"
echo "========================================================================"
echo ""

echo "JOB INFORMATION:"
echo "   Job ID:    $SLURM_JOB_ID"
echo "   Node:      $SLURM_NODELIST"
echo "   GPUs:      $SLURM_GPUS"
echo "   CPUs:      $SLURM_CPUS_PER_TASK"
echo ""

echo "CHECKPOINT RESUMING:"
if [ -n "$RESUME_CHECKPOINT" ]; then
    echo "   Status:      RESUMING FROM CHECKPOINT"
    echo "   Checkpoint:  $RESUME_CHECKPOINT"
    if [ -f "$RESUME_CHECKPOINT" ]; then
        echo "   File exists: YES"
    else
        echo "   File exists: NO  <-- WARNING: file not found!"
    fi
    if [ -n "$RESUME_EPOCH" ]; then
        echo "   Epoch override: $RESUME_EPOCH (--resume_epoch)"
    else
        echo "   Epoch override: auto (resumes at saved_epoch + 1)"
    fi
else
    echo "   Status:      FRESH TRAINING (no checkpoint)"
fi
echo ""

echo "SEMANTIC LEARNING:"
if (( $(echo "$SEGMENT_WEIGHT > 0" | bc -l) )) || \
   (( $(echo "$INSTANCE_WEIGHT > 0" | bc -l) )); then
    echo "   Status:         ENABLED"
    echo "   Mode:           ${SEMANTIC_MODE^^}"
    echo "   Segment weight: $SEGMENT_WEIGHT"
    echo "   Instance weight:$INSTANCE_WEIGHT"
else
    echo "   Status:         DISABLED (Baseline)"
fi
echo ""

echo "SCALE NORMALIZATION:"
echo "   Mode: $SCALE_NORM_MODE"
echo ""

echo "COLOR CONFIGURATION:"
echo "   Color Loss Weight: ${COLOR_LOSS_WEIGHT}x"
echo ""

echo "TRAINING HYPERPARAMETERS:"
echo "   Batch Size:  $BATCH_SIZE"
echo "   Epochs:      $NUM_EPOCHS"
echo "   LR:          $LEARNING_RATE"
echo "   KL Weight:   $KL_WEIGHT"
echo "   Eval Every:  $EVAL_EVERY"
echo ""

echo "DATASET:"
echo "   Train Scenes: $TRAIN_SCENES"
echo "   Val Scenes:   $VAL_SCENES"
echo "   Sampling:     $SAMPLING_METHOD"
echo "   Canonical:    $USE_CANONICAL_NORM"
echo ""

echo "========================================================================"
echo ""

# ============================================================================
# ENVIRONMENT SETUP
# ============================================================================

echo "Setting up environment..."
module purge && module load 2023 && module load CUDA/12.1.1
export PATH="/home/yli11/.conda/envs/can3tok/bin:$PATH"
export LD_LIBRARY_PATH="/home/yli11/.conda/envs/can3tok/lib/python3.11/\
site-packages/torch/lib:$LD_LIBRARY_PATH"

cd /home/yli11/scratch/Hafeez_thesis/Can3Tok
mkdir -p logs checkpoints

echo "   Modules loaded"
echo "   Conda environment: can3tok"
echo "   Working directory: $(pwd)"
echo ""

if [ "$USE_WANDB" = "True" ]; then
    wandb login $WANDB_API_KEY --relogin 2>/dev/null
    echo "   Weights & Biases authenticated"
else
    echo "   Weights & Biases disabled"
fi
echo ""

# ============================================================================
# BUILD TRAINING COMMAND
# ============================================================================

echo "Building training command..."

TRAIN_CMD="python gs_can3tok_2.py"

# Training parameters
TRAIN_CMD="$TRAIN_CMD --batch_size $BATCH_SIZE"
TRAIN_CMD="$TRAIN_CMD --num_epochs $NUM_EPOCHS"
TRAIN_CMD="$TRAIN_CMD --lr $LEARNING_RATE"
TRAIN_CMD="$TRAIN_CMD --kl_weight $KL_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --eval_every $EVAL_EVERY"
TRAIN_CMD="$TRAIN_CMD --failure_threshold $FAILURE_THRESHOLD"

# Dataset parameters
TRAIN_CMD="$TRAIN_CMD --train_scenes $TRAIN_SCENES"
TRAIN_CMD="$TRAIN_CMD --val_scenes $VAL_SCENES"
TRAIN_CMD="$TRAIN_CMD --sampling_method $SAMPLING_METHOD"

# Semantic parameters
TRAIN_CMD="$TRAIN_CMD --semantic_mode $SEMANTIC_MODE"
TRAIN_CMD="$TRAIN_CMD --segment_loss_weight $SEGMENT_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --instance_loss_weight $INSTANCE_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --semantic_temperature $SEMANTIC_TEMP"
TRAIN_CMD="$TRAIN_CMD --semantic_subsample $SEMANTIC_SUBSAMPLE"
TRAIN_CMD="$TRAIN_CMD --sampling_strategy $SAMPLING_STRATEGY"

# Color loss weighting
TRAIN_CMD="$TRAIN_CMD --color_loss_weight $COLOR_LOSS_WEIGHT"

# Scale normalization mode
TRAIN_CMD="$TRAIN_CMD --scale_norm_mode $SCALE_NORM_MODE"

# PCA visualization
TRAIN_CMD="$TRAIN_CMD --pca_vis_freq $PCA_VIS_FREQ"
TRAIN_CMD="$TRAIN_CMD --pca_num_scenes $PCA_NUM_SCENES"
TRAIN_CMD="$TRAIN_CMD --pca_brightness $PCA_BRIGHTNESS"

# 3DGS reconstruction
TRAIN_CMD="$TRAIN_CMD --recon_ply_freq $RECON_PLY_FREQ"
TRAIN_CMD="$TRAIN_CMD --recon_ply_num_scenes $RECON_PLY_NUM_SCENES"
TRAIN_CMD="$TRAIN_CMD --recon_ply_max_sh $RECON_PLY_MAX_SH"

# Normalization flags
if [ "$USE_CANONICAL_NORM" = "False" ]; then
    TRAIN_CMD="$TRAIN_CMD --no_canonical_norm"
fi

# W&B
if [ "$USE_WANDB" = "True" ]; then
    TRAIN_CMD="$TRAIN_CMD --use_wandb"
    TRAIN_CMD="$TRAIN_CMD --wandb_project Can3Tok-Semantic"
    TRAIN_CMD="$TRAIN_CMD --wandb_entity 3D-SSC"
fi

# ─── CHECKPOINT RESUMING ────────────────────────────────────────────────────
# Appended last so it is easy to spot in the printed command.
if [ -n "$RESUME_CHECKPOINT" ]; then
    TRAIN_CMD="$TRAIN_CMD --resume_checkpoint $RESUME_CHECKPOINT"
    if [ -n "$RESUME_EPOCH" ]; then
        TRAIN_CMD="$TRAIN_CMD --resume_epoch $RESUME_EPOCH"
    fi
fi
# ────────────────────────────────────────────────────────────────────────────

echo "   Command:"
echo "   $TRAIN_CMD"
echo ""

# ============================================================================
# START TRAINING
# ============================================================================

echo "========================================================================"
echo "                       STARTING TRAINING"
echo "========================================================================"
echo ""

SECONDS=0

eval $TRAIN_CMD
TRAIN_EXIT=$?

DURATION=$((SECONDS / 60))

echo ""
echo "========================================================================"
echo "                       TRAINING COMPLETE"
echo "========================================================================"
echo "Exit code: $TRAIN_EXIT"
echo "Duration:  $DURATION minutes"
echo "End time:  $(date)"
echo ""

if [ $TRAIN_EXIT -eq 0 ]; then
    echo "Training completed successfully!"
    echo ""
    echo "Next steps:"
    echo "   1. Check logs:   cat logs/can3tok_${SLURM_JOB_ID}.out"
    echo "   2. Checkpoints:  ls checkpoints/RGB_job_${SLURM_JOB_ID}_*/"
    echo "   3. Best model:   checkpoints/RGB_job_${SLURM_JOB_ID}_*/best_model.pth"
    if [ "$USE_WANDB" = "True" ]; then
        echo "   4. W&B:          https://wandb.ai/3D-SSC/Can3Tok-Semantic"
    fi
else
    echo "Training FAILED with exit code $TRAIN_EXIT"
    echo ""
    echo "Troubleshooting:"
    echo "   1. Error log:  cat logs/can3tok_${SLURM_JOB_ID}.err"
    echo "   2. Check GPU:  nvidia-smi"
    echo "   3. Check path: ls $RESUME_CHECKPOINT"
fi

echo "========================================================================"