#!/bin/bash
#SBATCH --job-name=can3tok_semantic
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --time=12:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --mem=64G
#SBATCH --output=logs/bala_cb_sw1_hidden_t_0.5.out
#SBATCH --error=logs/can3tok_%j.err

# ============================================================================
# CAN3TOK TRAINING - FULL SEMANTIC CONTROL
# ============================================================================
# This job file gives you complete control over semantic learning via flags
# No need to edit YAML files - everything controlled here!
# ============================================================================

echo "================================================================================"
echo "               CAN3TOK TRAINING - SEMANTIC LEARNING CONTROL"
echo "================================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Start time: $(date)"
echo ""

# ============================================================================
# ğŸ¯ SEMANTIC LEARNING CONFIGURATION - CHOOSE YOUR OPTION!
# ============================================================================

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  UNCOMMENT THE OPTION YOU WANT TO USE                                   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPTION 1: NO SEMANTIC LEARNING (Baseline - Fastest, Least Memory)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use this for: Baseline comparisons, fastest training, minimal memory
# Memory: ~20GB | Speed: âš¡âš¡âš¡ Fastest | Quality: Standard reconstruction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# SEMANTIC_MODE="geometric"       # Value doesn't matter when disabled
# SEGMENT_WEIGHT=0.0              # â† KEY: Set to 0 to disable
# INSTANCE_WEIGHT=0.0             # â† KEY: Set to 0 to disable

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPTION 2: GEOMETRIC MODE (Recommended - Best Balance!)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use this for: Standard semantic learning, efficient training
# Memory: ~20GB | Speed: âš¡âš¡âš¡ Fast | Quality: Good semantic features
# Parameters: ~45K | Approach: Uses reconstructed Gaussian parameters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# SEMANTIC_MODE="geometric"
# SEGMENT_WEIGHT=0.1              # â† Set > 0 to enable
# INSTANCE_WEIGHT=0.0             # Optional: instance-level loss

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPTION 3: HIDDEN STATE MODE (Best Quality - Slowest)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use this for: Maximum semantic quality, rich feature learning
# Memory: ~35GB | Speed: âš¡âš¡ Medium | Quality: Best semantic features
# Parameters: ~329M | Approach: Uses decoder hidden state
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SEMANTIC_MODE="hidden"
SEGMENT_WEIGHT=1              # â† Set > 0 to enable
INSTANCE_WEIGHT=0.0             # Optional: instance-level loss

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPTION 4: ATTENTION MODE (Spatial Context)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use this for: Learning spatial relationships between Gaussians
# Memory: ~30GB | Speed: âš¡âš¡ Medium | Quality: Good spatial features
# Parameters: ~158M | Approach: Cross-attention between Gaussians and scene
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# SEMANTIC_MODE="attention"
# SEGMENT_WEIGHT=0.1              # â† Set > 0 to enable
# INSTANCE_WEIGHT=0.0             # Optional: instance-level loss

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Semantic Loss Additional Parameters (only used if SEGMENT_WEIGHT > 0)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEMANTIC_TEMP=0.5              # Temperature for InfoNCE loss
SEMANTIC_SUBSAMPLE=10000         # Subsample Gaussians (saves memory/compute)
SAMPLING_STRATEGY="balanced" # "random" or "balanced" sampling for semantic loss

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================

BATCH_SIZE=64
NUM_EPOCHS=100
LEARNING_RATE=1e-3
KL_WEIGHT=1e-4                 # VAE KL divergence weight
RECON_SCALE=1.0              # Scale reconstruction loss
EVAL_EVERY=10                   # Evaluate every N epochs
FAILURE_THRESHOLD=150          # L2 error threshold for failure

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

TRAIN_SCENES=600                # Number of training scenes (None = all)
VAL_SCENES=50                   # Number of validation scenes (None = all)
SAMPLING_METHOD="opacity"       # "opacity" or "random"

# ============================================================================
# WEIGHTS & BIASES CONFIGURATION
# ============================================================================

export WANDB_API_KEY="your_key_here"
USE_WANDB=False                 # Set to True to enable W&B logging

# ============================================================================
# PRINT CONFIGURATION SUMMARY
# ============================================================================

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                        CONFIGURATION SUMMARY"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Job Information
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ“‹ JOB INFORMATION:"
echo "   Job ID:            $SLURM_JOB_ID"
echo "   Partition:         $SLURM_JOB_PARTITION"
echo "   Node:              $SLURM_NODELIST"
echo "   GPUs:              $SLURM_GPUS"
echo "   CPUs:              $SLURM_CPUS_PER_TASK"
echo "   Memory:            64GB"
echo "   Time Limit:        12:00:00"
echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Semantic Learning Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ§  SEMANTIC LEARNING:"

# Check if semantic learning is enabled
if (( $(echo "$SEGMENT_WEIGHT > 0" | bc -l) )) || (( $(echo "$INSTANCE_WEIGHT > 0" | bc -l) )); then
    echo "   Status:            âœ… ENABLED"
    echo "   Mode:              ${SEMANTIC_MODE^^}"
    echo ""
    echo "   Loss Weights:"
    echo "      Segment (Î²):    $SEGMENT_WEIGHT"
    echo "      Instance (Î³):   $INSTANCE_WEIGHT"
    echo "      Temperature:    $SEMANTIC_TEMP"
    echo "      Subsample:      $SEMANTIC_SUBSAMPLE Gaussians"
    echo ""
    
    # Mode-specific information
    case "$SEMANTIC_MODE" in
        "geometric")
            echo "   Architecture Details:"
            echo "      Approach:       Geometric Gaussians â†’ MLP (SimCLR-style)"
            echo "      Input:          [B, 40k, 11] reconstructed Gaussians"
            echo "      Output:         [B, 40k, 32] semantic features"
            echo "      Parameters:     ~45K (very lightweight!)"
            echo "      Memory Usage:   ~20GB GPU"
            echo "      Training Speed: âš¡âš¡âš¡ Fast"
            echo "      Best For:       â­ Standard semantic learning"
            echo "      Notes:          Most efficient, recommended for most cases"
            ;;
        "hidden")
            echo "   Architecture Details:"
            echo "      Approach:       Hidden State â†’ MLP projection"
            echo "      Input:          [B, 1024] decoder hidden state"
            echo "      Output:         [B, 40k, 32] semantic features"
            echo "      Parameters:     ~329M (heavyweight)"
            echo "      Memory Usage:   ~35GB GPU"
            echo "      Training Speed: âš¡âš¡ Medium"
            echo "      Best For:       Maximum semantic quality"
            echo "      Notes:          Richest features, but slower training"
            ;;
        "attention")
            echo "   Architecture Details:"
            echo "      Approach:       Cross-attention semantic head"
            echo "      Input:          [B, 40k, 3] positions Ã— [B, 512, 384] tokens"
            echo "      Output:         [B, 40k, 32] semantic features"
            echo "      Parameters:     ~158M (medium weight)"
            echo "      Memory Usage:   ~30GB GPU"
            echo "      Training Speed: âš¡âš¡ Medium"
            echo "      Best For:       Spatial semantic relationships"
            echo "      Notes:          Good for capturing scene context"
            ;;
        *)
            echo "      âš ï¸  Unknown semantic mode: $SEMANTIC_MODE"
            ;;
    esac
    
    echo ""
    echo "   Loss Composition:"
    echo "      Total = (Recon / $RECON_SCALE) + ($KL_WEIGHT Ã— KL) + Semantic"
    echo "      Semantic = ($SEGMENT_WEIGHT Ã— Segment) + ($INSTANCE_WEIGHT Ã— Instance)"
    
else
    # Semantic learning is disabled
    echo "   Status:            âŒ DISABLED (Baseline)"
    echo "   Reason:            All semantic loss weights = 0"
    echo ""
    echo "   Benefits:"
    echo "      Memory Saved:   ~15GB GPU (no semantic head)"
    echo "      Speed Gain:     ~30% faster training"
    echo "      Use Case:       â­ Baseline comparisons, pure VAE"
    echo ""
    echo "   Loss Composition:"
    echo "      Total = (Recon / $RECON_SCALE) + ($KL_WEIGHT Ã— KL)"
    echo "      (No semantic loss)"
fi

echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Training Hyperparameters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "âš™ï¸  TRAINING HYPERPARAMETERS:"
echo "   Batch Size:        $BATCH_SIZE"
echo "   Epochs:            $NUM_EPOCHS"
echo "   Learning Rate:     $LEARNING_RATE"
echo "   KL Weight:         $KL_WEIGHT"
echo "   Recon Scale:       $RECON_SCALE"
echo "   Eval Every:        $EVAL_EVERY epochs"
echo "   Failure Thresh:    $FAILURE_THRESHOLD"
echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Dataset Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ“Š DATASET:"
echo "   Training Scenes:   $TRAIN_SCENES"
echo "   Val Scenes:        $VAL_SCENES"
echo "   Sampling:          ${SAMPLING_METHOD^^}"
echo "   Dataset:           SceneSplat-7K (ScanNet72 labels)"
echo "   Semantic Classes:  72 categories (38 main + 34 rare)"
echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ“ˆ LOGGING:"
echo "   W&B:               $([ "$USE_WANDB" = "True" ] && echo "âœ… Enabled" || echo "âŒ Disabled")"
echo "   Checkpoints:       Every 10 epochs"
echo "   Best Model:        Tracked by val L2 error"
echo "   Output Log:        logs/can3tok_${SLURM_JOB_ID}.out"
echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Expected Performance
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ¯ EXPECTED PERFORMANCE:"
echo "   Initial L2:        ~19,000 (random)"
echo "   Target L2:         ~3,000-5,000 (converged)"
echo "   Failure if:        L2 > $FAILURE_THRESHOLD"

if (( $(echo "$SEGMENT_WEIGHT > 0" | bc -l) )); then
    echo ""
    echo "   Semantic Loss:"
    echo "      Initial:        ~4.28 (log(72), random features)"
    echo "      Target:         ~0.5-1.5 (learned clusters)"
    echo "      âš ï¸  Watch:       Should decrease steadily"
fi

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# ============================================================================
# ENVIRONMENT SETUP
# ============================================================================

echo "ğŸ”§ Setting up environment..."
module purge && module load 2023 && module load CUDA/12.1.1
export PATH="/home/yli11/.conda/envs/can3tok/bin:$PATH"
export LD_LIBRARY_PATH="/home/yli11/.conda/envs/can3tok/lib/python3.11/site-packages/torch/lib:$LD_LIBRARY_PATH"

cd /home/yli11/scratch/Hafeez_thesis/Can3Tok
mkdir -p logs checkpoints

echo "   âœ“ Modules loaded"
echo "   âœ“ Conda environment: can3tok"
echo "   âœ“ Working directory: $(pwd)"
echo ""

# W&B setup
if [ "$USE_WANDB" = "True" ]; then
    wandb login $WANDB_API_KEY --relogin 2>/dev/null
    echo "   âœ“ Weights & Biases authenticated"
else:
    echo "   âŠ˜ Weights & Biases disabled"
fi
echo ""

# ============================================================================
# BUILD TRAINING COMMAND
# ============================================================================

echo "ğŸš€ Building training command..."

TRAIN_CMD="python  gs_can3tok_2.py"

# Training parameters
TRAIN_CMD="$TRAIN_CMD --batch_size $BATCH_SIZE"
TRAIN_CMD="$TRAIN_CMD --num_epochs $NUM_EPOCHS"
TRAIN_CMD="$TRAIN_CMD --lr $LEARNING_RATE"
TRAIN_CMD="$TRAIN_CMD --kl_weight $KL_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --eval_every $EVAL_EVERY"
TRAIN_CMD="$TRAIN_CMD --failure_threshold $FAILURE_THRESHOLD"
TRAIN_CMD="$TRAIN_CMD --recon_scale $RECON_SCALE"

# Dataset parameters
TRAIN_CMD="$TRAIN_CMD --train_scenes $TRAIN_SCENES"
TRAIN_CMD="$TRAIN_CMD --val_scenes $VAL_SCENES"
TRAIN_CMD="$TRAIN_CMD --sampling_method $SAMPLING_METHOD"

# ============================================================================
# SEMANTIC PARAMETERS - THESE CONTROL EVERYTHING!
# ============================================================================
# The training script will automatically:
#   - Disable semantic learning if weights are 0
#   - Enable semantic learning if weights > 0
#   - Initialize only the selected semantic head
# ============================================================================

TRAIN_CMD="$TRAIN_CMD --semantic_mode $SEMANTIC_MODE"
TRAIN_CMD="$TRAIN_CMD --segment_loss_weight $SEGMENT_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --instance_loss_weight $INSTANCE_WEIGHT"
TRAIN_CMD="$TRAIN_CMD --semantic_temperature $SEMANTIC_TEMP"
TRAIN_CMD="$TRAIN_CMD --semantic_subsample $SEMANTIC_SUBSAMPLE"
TRAIN_CMD="$TRAIN_CMD --sampling_strategy $SAMPLING_STRATEGY"

# W&B parameters
if [ "$USE_WANDB" = "True" ]; then
    TRAIN_CMD="$TRAIN_CMD --use_wandb"
    TRAIN_CMD="$TRAIN_CMD --wandb_project Can3Tok-Semantic"
    TRAIN_CMD="$TRAIN_CMD --wandb_entity 3D-SSC"
fi

echo "Command:"
echo "$TRAIN_CMD"
echo ""

# ============================================================================
# START TRAINING
# ============================================================================

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                          STARTING TRAINING"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

SECONDS=0

eval $TRAIN_CMD
TRAIN_EXIT=$?

DURATION=$((SECONDS / 60))

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                          TRAINING COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Exit code:     $TRAIN_EXIT"
echo "Duration:      $DURATION minutes"
echo "End time:      $(date)"
echo ""

if [ $TRAIN_EXIT -eq 0 ]; then
    echo "âœ… Training completed successfully!"
    echo ""
    echo "Next steps:"
    echo "   1. Check logs:           cat logs/can3tok_${SLURM_JOB_ID}.out"
    echo "   2. Find checkpoints:     ls checkpoints/job_${SLURM_JOB_ID}_*/"
    echo "   3. Load best model:      checkpoints/job_${SLURM_JOB_ID}_*/best_model.pth"
    if [ "$USE_WANDB" = "True" ]; then
        echo "   4. View W&B dashboard:   https://wandb.ai/3D-SSC/Can3Tok-Semantic"
    fi
else
    echo "âŒ Training failed with exit code $TRAIN_EXIT"
    echo ""
    echo "Troubleshooting:"
    echo "   1. Check error log:      cat logs/can3tok_${SLURM_JOB_ID}.err"
    echo "   2. Check GPU memory:     nvidia-smi"
    echo "   3. Verify dataset path:  ls /home/yli11/scratch/datasets/gaussian_world/preprocessed/interior_gs/"
fi

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"