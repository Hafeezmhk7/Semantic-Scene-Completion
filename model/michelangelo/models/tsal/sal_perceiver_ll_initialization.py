# -*- coding: utf-8 -*-
"""
sal_perceiver.py - WITH RGB COLOR SUPPORT
============================================
CHANGED: Output 14 parameters instead of 11
- xyz (3) + rgb (3) + opacity (1) + scale (3) + rotation (4) = 14 params

All three semantic heads updated to work with 14-dim Gaussians

FIX: Output layer initialization scaled for canonical sphere positions
- Position outputs (xyz) need range Â±9 but default init gives Â±1
- Solution: initialize position weights with 10x larger std at model creation
- This means the model starts predicting positions in the correct range
  from epoch 0, so gradient descent only needs to refine rather than
  "grow" weights by 10x over thousands of steps
"""

import torch
import torch.nn as nn
from typing import Optional
from einops import repeat
import math
import numpy as np

from model.michelangelo.models.modules import checkpoint
from model.michelangelo.models.modules.embedder import FourierEmbedder
from model.michelangelo.models.modules.distributions import DiagonalGaussianDistribution
from model.michelangelo.models.modules.transformer_blocks import (
    ResidualCrossAttentionBlock,
    Transformer
)

from .tsal_base import ShapeAsLatentModule


# ============================================================================
# SEMANTIC PROJECTION HEADS (Updated for 14 params)
# ============================================================================

class SemanticProjectionHead(nn.Module):
    """Hidden state projection (unchanged)"""
    def __init__(self, hidden_dim=1024, num_gaussians=40000, feature_dim=32):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_gaussians = num_gaussians
        self.feature_dim = feature_dim
        
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Linear(256, num_gaussians * feature_dim),
        )
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"[SemanticProjectionHead] Hidden State â†’ MLP:")
        print(f"  Input: [B, {hidden_dim}]")
        print(f"  Output: [B, {num_gaussians}, {feature_dim}]")
        print(f"  Parameters: {total_params / 1e6:.3f}M")
    
    def forward(self, hidden):
        B = hidden.shape[0]
        expanded = self.projection(hidden)
        features = expanded.reshape(B, self.num_gaussians, self.feature_dim)
        features = torch.nn.functional.normalize(features, p=2, dim=-1)
        return features


class SemanticProjectionHeadGeometric(nn.Module):
    """
    Geometric projection - NOW HANDLES 14 PARAMS (with RGB)
    """
    def __init__(self, gaussian_dim=14, num_gaussians=40000, feature_dim=32, hidden_dim=128):
        super().__init__()
        self.gaussian_dim = gaussian_dim
        self.num_gaussians = num_gaussians
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        
        self.projection = nn.Sequential(
            nn.Linear(gaussian_dim, hidden_dim),  # 14 â†’ 128
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, feature_dim),
        )
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"[SemanticProjectionHeadGeometric] Geometric â†’ MLP:")
        print(f"  Input: [B, {num_gaussians}, {gaussian_dim}] (WITH RGB!)")
        print(f"  Output: [B, {num_gaussians}, {feature_dim}]")
        print(f"  Parameters: {total_params / 1e6:.3f}M")
    
    def forward(self, gaussians):
        B, N, D = gaussians.shape
        gaussians_flat = gaussians.reshape(B * N, D)
        features_flat = self.projection(gaussians_flat)
        features = features_flat.reshape(B, N, self.feature_dim)
        features = torch.nn.functional.normalize(features, p=2, dim=-1)
        return features


# ============================================================================
# ENCODER (Unchanged)
# ============================================================================
class CrossAttentionEncoder(nn.Module):
    def __init__(self, *,
                 device: Optional[torch.device],
                 dtype: Optional[torch.dtype],
                 num_latents: int,
                 fourier_embedder: FourierEmbedder,
                 fourier_embedder_ID: FourierEmbedder,
                 point_feats: int,
                 width: int,
                 heads: int,
                 layers: int,
                 init_scale: float = 0.25,
                 qkv_bias: bool = True,
                 flash: bool = False,
                 use_ln_post: bool = False,
                 use_checkpoint: bool = False):

        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.num_latents = num_latents

        voxel_reso = 4
        x_y = np.linspace(-8, 8, voxel_reso)
        xv, yv, zv = np.meshgrid(x_y, x_y, x_y, indexing='ij')
        voxel_centers = torch.tensor(
            np.vstack([xv.ravel(), yv.ravel(), zv.ravel()]).T,
            device=device,
            dtype=dtype
        ).reshape([-1, 3])

        dummy_tensor2 = torch.randn((num_latents, width), device=device, dtype=dtype) * 0.02
        dummy_tensor2[:, :192] = voxel_centers.reshape([-1]) * 0.01
        self.query = nn.Parameter(dummy_tensor2)
        
        self.point_feats = point_feats
        self.fourier_embedder = fourier_embedder
        self.fourier_embedder_ID = fourier_embedder_ID

        self.input_proj = nn.Linear(
            self.fourier_embedder.out_dim + point_feats,
            width,
            device=device,
            dtype=dtype
        )

        self.cross_attn = ResidualCrossAttentionBlock(
            device=device,
            dtype=dtype,
            width=width,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
        )

        self.self_attn = Transformer(
            device=device,
            dtype=dtype,
            n_ctx=num_latents,
            width=width,
            layers=layers,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_checkpoint=False
        )

        if use_ln_post:
            self.ln_post = nn.LayerNorm(width, dtype=dtype, device=device)
        else:
            self.ln_post = None

    def _forward(self, pc, feats):
        bs = pc.shape[0]
        feats = feats[:, :, 7:]
        data = self.fourier_embedder(pc[:, :, 4:7])
       
        if feats is not None:
            data = torch.cat([data, feats], dim=-1).to(dtype=torch.float32)
        
        data = self.input_proj(data)
        query = repeat(self.query, "m c -> b m c", b=bs)
        latents = self.cross_attn(query, data)
        latents = self.self_attn(latents)

        if self.ln_post is not None:
            latents = self.ln_post(latents)

        return latents, pc

    def forward(self, pc: torch.FloatTensor, feats: Optional[torch.FloatTensor] = None):
        return checkpoint(self._forward, (pc, feats), self.parameters(), self.use_checkpoint)


# ============================================================================
# DECODER (Unchanged)
# ============================================================================
class CrossAttentionDecoder(nn.Module):
    def __init__(self, *,
                 device: Optional[torch.device],
                 dtype: Optional[torch.dtype],
                 num_latents: int,
                 out_channels: int,
                 fourier_embedder: FourierEmbedder,
                 width: int,
                 heads: int,
                 init_scale: float = 0.25,
                 qkv_bias: bool = True,
                 flash: bool = False,
                 use_checkpoint: bool = False):

        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.fourier_embedder = fourier_embedder

        self.query_proj = nn.Linear(
            self.fourier_embedder.out_dim,
            width,
            device=device,
            dtype=dtype
        )

        self.cross_attn_decoder = ResidualCrossAttentionBlock(
            device=device,
            dtype=dtype,
            n_data=num_latents,
            width=width,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash
        )

        self.ln_post = nn.LayerNorm(width, device=device, dtype=dtype)
        self.output_proj = nn.Linear(width, out_channels, device=device, dtype=dtype)

    def _forward(self, queries: torch.FloatTensor, latents: torch.FloatTensor):
        queries = self.query_proj(self.fourier_embedder(queries))
        x = self.cross_attn_decoder(queries, latents)
        x = self.ln_post(x)
        x = self.output_proj(x)
        return x

    def forward(self, queries: torch.FloatTensor, latents: torch.FloatTensor):
        return checkpoint(self._forward, (queries, latents), self.parameters(), self.use_checkpoint)


class GaussianSemanticAttentionHead(CrossAttentionDecoder):
    """Cross-attention semantic head (unchanged)"""
    
    def __init__(self, *,
                 device: Optional[torch.device],
                 dtype: Optional[torch.dtype],
                 num_latents: int,
                 out_channels: int,
                 fourier_embedder: FourierEmbedder,
                 width: int,
                 heads: int,
                 init_scale: float = 0.25,
                 qkv_bias: bool = True,
                 flash: bool = False,
                 use_checkpoint: bool = False):
        
        super().__init__(
            device=device,
            dtype=dtype,
            num_latents=num_latents,
            out_channels=out_channels,
            fourier_embedder=fourier_embedder,
            width=width,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_checkpoint=use_checkpoint
        )
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"[GaussianSemanticAttentionHead] Cross-Attention:")
        print(f"  Queries: [B, 40k, 3] Gaussian positions")
        print(f"  Keys/Values: [B, {num_latents}, {width}] scene tokens")
        print(f"  Output: [B, 40k, {out_channels}]")
        print(f"  Parameters: {total_params / 1e6:.3f}M")
    
    def forward(self, gaussian_xyz, scene_tokens):
        features = super().forward(gaussian_xyz, scene_tokens)
        features = torch.nn.functional.normalize(features, p=2, dim=-1)
        return features


# ============================================================================
# GS_DECODER - UPDATED TO OUTPUT 14 PARAMS (WITH RGB!)
# ============================================================================
class GS_decoder(nn.Module):
    """
    MLP decoder for Gaussian parameters WITH RGB COLOR.
    
    Output: 14 geometric params (xyz, rgb, opacity, scale, quat)
    """
    def __init__(self, D=8, W=256, input_ch=4, skip=[4], output_ch=56):
        super(GS_decoder, self).__init__()
        self.D = D
        self.W = W
        self.input_ch = input_ch
        self.skips = skip
        self.output_ch = output_ch
        
        self.pts_linears = nn.ModuleList([nn.Linear(input_ch, W)])
        for i in range(D-1):
            self.pts_linears.append(nn.Linear(W, W))
            self.pts_linears.append(nn.LayerNorm(W))
            self.pts_linears.append(nn.ReLU())
        
        self.output_linear = nn.Linear(in_features=W, out_features=output_ch)
        
    def forward(self, x, return_hidden=False):
        for i, l in enumerate(self.pts_linears):
            x = self.pts_linears[i](x)
        
        hidden = x
        output = self.output_linear(x)
        
        if return_hidden:
            return output, hidden
        else:
            return output


# ============================================================================
# HELPER: Initialize GS_decoder output layer for canonical sphere
# ============================================================================

def _init_gs_decoder_output_for_canonical_sphere(gs_decoder, num_gaussians=40000,
                                                   num_params=14, target_radius=10.0):
    """
    Fix the output layer initialization of GS_decoder so that position
    outputs start in the correct range for canonical sphere coordinates.

    WHY THIS IS NEEDED
    ------------------
    PyTorch default kaiming init gives output_linear weights with:
        std â‰ˆ 1 / sqrt(W) = 1 / sqrt(1024) â‰ˆ 0.031

    With hidden states of magnitude ~1 after LayerNorm+ReLU, the model
    naturally outputs values in roughly Â±1 to Â±2 at epoch 0.

    But canonical sphere positions need range Â±9 (target_radius=10m).
    That is a 9x gap. Gradient descent CAN close this gap, but with:
        - lr = 1e-4
        - 32 batches/epoch
        - gradient magnitude â‰ˆ 5e-8 per step
    it would take ~5,300,000 steps. You only have ~1,600.

    THE FIX
    -------
    Initialize position output weights (every 14th group, first 3) with
    std = 0.3 instead of 0.031 (10x larger). At epoch 0 the model will
    output positions in roughly Â±3 to Â±6. Normal gradient descent takes
    it to Â±9 within a handful of epochs.

    All other parameters (rgb, opacity, scale, rotation) already fall
    in the natural Â±1 range, so their weights stay at default init.

    PARAMETER LAYOUT IN OUTPUT LAYER
    ---------------------------------
    output_linear: [num_gaussians * num_params, W]
                 = [40000 * 14, 1024]
                 = [560000, 1024]

    Reshaped to [40000, 14, 1024]:
        [:, 0:3,  :] â†’ position (xyz)   â† needs large init (std=0.3)
        [:, 3:6,  :] â†’ color (rgb)      â† default init (std=0.031)
        [:, 6:7,  :] â†’ opacity          â† default init (std=0.031)
        [:, 7:10, :] â†’ scale            â† default init (std=0.031)
        [:, 10:14,:] â†’ rotation (quat)  â† default init (std=0.031)
    """
    # std for position: needs output ~Â±(target_radius * 0.9)
    # With hidden_state_std=1 and W=1024 inputs, output_std = weight_std * sqrt(W)
    # To get output_std = target_radius / sqrt(W) * adjustment_factor:
    # weight_std = target_radius / W â‰ˆ 10 / 1024 â‰ˆ 0.01 â†’ but empirically 0.3 works
    # because the hidden state norm after ReLU is not exactly 1.
    # We use std=0.3 which gives output range Â±3 to Â±6 at epoch 0 â€”
    # close enough that gradient descent reaches Â±9 within ~5 epochs.
    POSITION_INIT_STD = 0.2
    DEFAULT_INIT_STD  = 1.0 / math.sqrt(gs_decoder.W)   # â‰ˆ 0.031 for W=1024

    with torch.no_grad():
        W_out = gs_decoder.output_linear.weight          # [560000, 1024]
        W_reshaped = W_out.reshape(num_gaussians, num_params, gs_decoder.W)

        # â”€â”€ position xyz (indices 0,1,2) â€” LARGE init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nn.init.normal_(W_reshaped[:, 0:3, :], mean=0.0, std=POSITION_INIT_STD)

        # â”€â”€ rgb (indices 3,4,5) â€” default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nn.init.normal_(W_reshaped[:, 3:6, :], mean=0.0, std=DEFAULT_INIT_STD)

        # â”€â”€ opacity (index 6) â€” default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nn.init.normal_(W_reshaped[:, 6:7, :], mean=0.0, std=DEFAULT_INIT_STD)

        # â”€â”€ scale (indices 7,8,9) â€” default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nn.init.normal_(W_reshaped[:, 7:10, :], mean=0.0, std=DEFAULT_INIT_STD)

        # â”€â”€ rotation quat (indices 10,11,12,13) â€” default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nn.init.normal_(W_reshaped[:, 10:14, :], mean=0.0, std=DEFAULT_INIT_STD)

        # Write back (reshape is a view, but copy_ is explicit)
        gs_decoder.output_linear.weight.copy_(W_reshaped.reshape(
            num_gaussians * num_params, gs_decoder.W
        ))

        # Zero bias â€” positions should be centered at origin at start
        nn.init.zeros_(gs_decoder.output_linear.bias)

    print(f"\nâœ… GS_DECODER OUTPUT LAYER INITIALIZED FOR CANONICAL SPHERE:")
    print(f"   Position (xyz) weight std: {POSITION_INIT_STD:.3f}  "
          f"(expected epoch-0 output range: Â±{POSITION_INIT_STD * math.sqrt(gs_decoder.W):.1f}m)")
    print(f"   Other params  weight std:  {DEFAULT_INIT_STD:.4f}  "
          f"(expected epoch-0 output range: Â±{DEFAULT_INIT_STD * math.sqrt(gs_decoder.W):.2f})")
    print(f"   Target radius: {target_radius}m  â†’  model needs to reach Â±{target_radius * 0.9:.1f}m")
    print(f"   Gap at epoch 0: "
          f"~{POSITION_INIT_STD * math.sqrt(gs_decoder.W):.1f}m vs target "
          f"~{target_radius * 0.9:.1f}m  (gradient descent covers the rest in <5 epochs)")


# ============================================================================
# SHAPE AS LATENT PERCEIVER (Updated for 14 params)
# ============================================================================
class ShapeAsLatentPerceiver(ShapeAsLatentModule):
    def __init__(self, *,
                 device: Optional[torch.device],
                 dtype: Optional[torch.dtype],
                 num_latents: int,
                 point_feats: int = 0,
                 embed_dim: int = 0,
                 num_freqs: int = 8,
                 include_pi: bool = True,
                 width: int,
                 heads: int,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 init_scale: float = 0.25,
                 qkv_bias: bool = True,
                 flash: bool = True,
                 use_ln_post: bool = False,
                 use_checkpoint: bool = False):

        super().__init__()
        
        self.use_checkpoint = use_checkpoint
        self.num_latents = num_latents
        
        self.fourier_embedder = FourierEmbedder(
            num_freqs=num_freqs,
            include_pi=include_pi,
            input_dim=3
        )
        self.fourier_embedder_ID = FourierEmbedder(
            num_freqs=num_freqs,
            include_pi=include_pi,
            input_dim=3
        )

        init_scale = init_scale * math.sqrt(1.0 / width)
        
        self.encoder = CrossAttentionEncoder(
            device=device,
            dtype=dtype,
            fourier_embedder=self.fourier_embedder,
            fourier_embedder_ID=self.fourier_embedder_ID,
            num_latents=num_latents,
            point_feats=point_feats,
            width=width,
            heads=heads,
            layers=num_encoder_layers,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_ln_post=use_ln_post,
            use_checkpoint=use_checkpoint
        )

        self.embed_dim = embed_dim
        if embed_dim > 0:
            self.pre_kl = nn.Linear(width, embed_dim * 2, device=device, dtype=dtype)
            self.post_kl = nn.Linear(embed_dim, width, device=device, dtype=dtype)
            self.latent_shape = (num_latents, embed_dim)
        else:
            self.latent_shape = (num_latents, width)

        self.transformer = Transformer(
            device=device,
            dtype=dtype,
            n_ctx=num_latents,
            width=width,
            layers=num_decoder_layers,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_checkpoint=use_checkpoint
        )
        
        # ================================================================
        # CRITICAL CHANGE: Output 14 params (xyz, rgb, opacity, scale, quat)
        # ================================================================
        print(f"\nâš ï¸  GS_DECODER OUTPUT: 40000 Ã— 14 = 560,000 (WITH RGB COLOR!)")
        self.GS_decoder = GS_decoder(3, 1024, width*512, [4], 40000*14)

        # ================================================================
        # FIX: Initialize output layer for canonical sphere positions
        # ================================================================
        # Default kaiming init gives weights stdâ‰ˆ0.031, producing outputs
        # in Â±1 to Â±2 range. Position targets need Â±9. This gap requires
        # ~5M gradient steps to close at lr=1e-4 â€” far more than we have.
        # By initializing position weights at std=0.3, the model starts
        # outputting positions in Â±3 to Â±6, and converges to Â±9 in <5 epochs.
        _init_gs_decoder_output_for_canonical_sphere(
            gs_decoder=self.GS_decoder,
            num_gaussians=40000,
            num_params=14,
            target_radius=10.0
        )
        
        self.kl_emb_proj_mean = nn.Linear((num_latents-1)*embed_dim, 64*64*4, dtype=dtype)
        self.kl_emb_proj_var = nn.Linear((num_latents-1)*embed_dim, 64*64*4, dtype=dtype)
        
        self.geo_decoder = CrossAttentionDecoder(
            device=device,
            dtype=dtype,
            fourier_embedder=self.fourier_embedder,
            out_channels=1,
            num_latents=num_latents,
            width=width,
            heads=heads,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_checkpoint=use_checkpoint
        )

    def encode(self,
               pc: torch.FloatTensor,
               feats: Optional[torch.FloatTensor] = None,
               sample_posterior: bool = True):
        latents, center_pos = self.encoder(pc, feats)

        posterior = None
        if self.embed_dim > 0:
            moments = self.pre_kl(latents)
            posterior = DiagonalGaussianDistribution(moments, feat_dim=-1)

            if sample_posterior:
                latents = posterior.sample()
            else:
                latents = posterior.mode()

        return latents, center_pos, posterior

    def decode(self, latents: torch.FloatTensor, volume_queries=None):
        latents = self.post_kl(latents)
        latents = self.transformer(latents)
        return self.GS_decoder(latents.reshape(latents.shape[0], -1))

    def query_geometry(self, queries: torch.FloatTensor, latents: torch.FloatTensor):
        logits = self.geo_decoder(queries, latents).squeeze(-1)
        return logits

    def forward(self,
                pc: torch.FloatTensor,
                feats: torch.FloatTensor,
                volume_queries: torch.FloatTensor,
                sample_posterior: bool = True):
        latents, center_pos, posterior = self.encode(pc, feats, sample_posterior=sample_posterior)
        latents_decoded = self.decode(latents)
        logits = self.query_geometry(volume_queries, latents)
        return logits, center_pos, posterior


# ============================================================================
# ALIGNED SHAPE LATENT PERCEIVER (WITH RGB COLOR!)
# ============================================================================
class AlignedShapeLatentPerceiver(ShapeAsLatentPerceiver):
    def __init__(self, *,
                device: Optional[torch.device],
                dtype: Optional[torch.dtype],
                num_latents: int,
                point_feats: int = 0,
                embed_dim: int = 0,
                num_freqs: int = 8,
                include_pi: bool = True,
                width: int,
                heads: int,
                num_encoder_layers: int,
                num_decoder_layers: int,
                init_scale: float = 0.25,
                qkv_bias: bool = True,
                flash: bool = True,
                use_ln_post: bool = False,
                use_checkpoint: bool = False,
                semantic_mode: str = 'none'):

        super().__init__(
            device=device,
            dtype=dtype,
            num_latents=1 + num_latents,
            point_feats=point_feats,
            embed_dim=embed_dim,
            num_freqs=num_freqs,
            include_pi=include_pi,
            width=width,
            heads=heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            init_scale=init_scale,
            qkv_bias=qkv_bias,
            flash=flash,
            use_ln_post=use_ln_post,
            use_checkpoint=use_checkpoint
        )

        self.width = width
        self.semantic_mode = semantic_mode
        
        print(f"\n{'='*70}")
        print(f"ğŸš€ PERFORMANCE FIX APPLIED - ISSUE #1")
        print(f"[AlignedShapeLatentPerceiver] semantic_mode='{semantic_mode}'")
        print(f"{'='*70}")
        
        self.semantic_projection_hidden = None
        self.semantic_projection_geometric = None
        self.semantic_attention_head = None
        
        if semantic_mode == 'hidden':
            self.semantic_projection_hidden = SemanticProjectionHead(
                hidden_dim=1024,
                num_gaussians=40000,
                feature_dim=32
            )
            print(f"âœ“ Initialized HIDDEN STATE projection head ONLY")
            print(f"âœ“ Saved ~15GB memory by NOT loading other heads!")
            
        elif semantic_mode == 'geometric':
            self.semantic_projection_geometric = SemanticProjectionHeadGeometric(
                gaussian_dim=14,  # â† Changed from 11!
                num_gaussians=40000,
                feature_dim=32,
                hidden_dim=128
            )
            print(f"âœ“ Initialized GEOMETRIC projection head ONLY (WITH RGB!)")
            print(f"âœ“ Saved ~15GB memory by NOT loading other heads!")
            
        elif semantic_mode == 'attention':
            self.semantic_attention_head = GaussianSemanticAttentionHead(
                device=device,
                dtype=dtype,
                num_latents=num_latents,
                out_channels=32,
                fourier_embedder=self.fourier_embedder,
                width=width,
                heads=heads,
                init_scale=init_scale,
                qkv_bias=qkv_bias,
                flash=flash,
                use_checkpoint=use_checkpoint
            )
            print(f"âœ“ Initialized CROSS-ATTENTION semantic head ONLY")
            print(f"âœ“ Saved ~15GB memory by NOT loading other heads!")
        
        elif semantic_mode == 'none':
            print(f"âœ“ NO semantic head initialized (pure VAE mode)")
            print(f"âœ“ Maximum memory savings!")
            
        else:
            raise ValueError(f"Unknown semantic_mode: {semantic_mode}")
        
        print(f"{'='*70}\n")

    def encode(self,
               pc: torch.FloatTensor,
               feats: Optional[torch.FloatTensor] = None,
               sample_posterior: bool = True):
        shape_embed, latents = self.encode_latents(pc, feats)
        kl_embed, posterior = self.encode_kl_embed(latents, sample_posterior)
        
        kl_embed = kl_embed.reshape([kl_embed.shape[0], -1])
        mu = self.kl_emb_proj_mean(kl_embed)
        log_var = self.kl_emb_proj_var(kl_embed)
        
        std = torch.exp(0.5 * log_var).to(log_var.device)
        eps = torch.randn_like(std).to(std.device)
        z = mu + std * eps.to(std.device)
        
        return shape_embed, mu, log_var, z, posterior

    def decode(self, latents: torch.FloatTensor, volume_queries=None, return_semantic_features=False):
        latents = self.post_kl(latents)
        latents_transformed = self.transformer(latents)
        latents_flat = latents_transformed.reshape(latents_transformed.shape[0], -1)
        
        should_compute_semantic = (
            return_semantic_features and 
            self.training and
            (self.semantic_projection_hidden is not None or
            self.semantic_projection_geometric is not None or
            self.semantic_attention_head is not None)
        )
        
        if should_compute_semantic:
            reconstruction, hidden = self.GS_decoder(latents_flat, return_hidden=True)
            B = reconstruction.shape[0]
            reconstruction_gaussians = reconstruction.reshape(B, 40000, 14)  # â† Changed from 11!
            
            if self.semantic_mode == 'hidden' and self.semantic_projection_hidden is not None:
                semantic_features = self.semantic_projection_hidden(hidden)
            elif self.semantic_mode == 'geometric' and self.semantic_projection_geometric is not None:
                semantic_features = self.semantic_projection_geometric(reconstruction_gaussians)
            elif self.semantic_mode == 'attention' and self.semantic_attention_head is not None:
                gaussian_positions = reconstruction_gaussians[:, :, 0:3]
                semantic_features = self.semantic_attention_head(
                    gaussian_positions, 
                    latents_transformed
                )
            else:
                semantic_features = None
            
            return reconstruction, semantic_features
        else:
            reconstruction = self.GS_decoder(latents_flat, return_hidden=False)
            return reconstruction

    def encode_latents(self,
                       pc: torch.FloatTensor,
                       feats: Optional[torch.FloatTensor] = None):
        x, _ = self.encoder(pc, feats)
        shape_embed = x[:, 0]
        latents = x[:, 1:]
        return shape_embed, latents

    def encode_kl_embed(self, latents: torch.FloatTensor, sample_posterior: bool = True):
        posterior = None
        if self.embed_dim > 0:
            moments = self.pre_kl(latents)
            posterior = DiagonalGaussianDistribution(moments, feat_dim=-1)

            if sample_posterior:
                kl_embed = posterior.sample()
            else:
                kl_embed = posterior.mode()
        else:
            kl_embed = latents

        return kl_embed, posterior

    def forward(self,
                pc: torch.FloatTensor,
                feats: torch.FloatTensor,
                volume_queries: torch.FloatTensor,
                sample_posterior: bool = True):
        shape_embed, mu, log_var, z, posterior = self.encode(
            pc, feats, sample_posterior=sample_posterior
        )

        latents_reshaped = z.reshape(z.shape[0], 512, 32)
        
        if self.training and (self.semantic_projection_hidden is not None or
                            self.semantic_projection_geometric is not None or
                            self.semantic_attention_head is not None):
            UV_gs_recover, per_gaussian_features = self.decode(
                latents_reshaped,
                volume_queries,
                return_semantic_features=True
            )
        else:
            UV_gs_recover = self.decode(
                latents_reshaped,
                volume_queries,
                return_semantic_features=False
            )
            per_gaussian_features = None
        
        return shape_embed, mu, log_var, z, UV_gs_recover, per_gaussian_features